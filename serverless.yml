# Welcome to Serverless!
#
# This file is the main config file for your service.
# It's very minimal at this point and uses default values.
# You can always add more config options for more control.
service: crawley-crawler

frameworkVersion: ">=1.28.0 <2.0.0"

provider:
  name: aws
  runtime: go1.x
  region: eu-west-2
  iamRoleStatements: # TODO: Segregate roles for functions.
    - Effect: "Allow"
      Action:
        - "sqs:SendMessage"
        - "sqs:GetQueueUrl"
      Resource: !GetAtt CrawlQueue.Arn
    - Effect: "Allow"
      Action:
        - "dynamodb:*"
      Resource: 
        - !GetAtt CrawlTable.Arn
        - !GetAtt CrawlSubscriptionTable.Arn
        - !Join [ "/", [ !GetAtt CrawlSubscriptionTable.Arn, "index", "CrawlIDIndex" ] ]

package:
  individually: true
  exclude:
   - ./**

functions:
  crawler:
    handler: bin/crawler
    package:
      include:
        - bin/crawler
    events:
      - sqs:
          arn:
            Fn::GetAtt:
              - CrawlQueue
              - Arn
    environment:
      CRAWL_TABLE_NAME: CrawlTable
      CRAWL_MAX_DEPTH: 3
      CRAWL_QUEUE_URL: !Ref CrawlQueue

  subscribe:
    handler: bin/subscribe
    package:
      include:
        - bin/subscribe
    events:
        - websocket:
            route: $connect
        - websocket:
            route: $disconnect
        - websocket:
            route: $default
        - websocket:
            route: subscribe
    environment:
      CRAWL_SUB_TABLE_NAME: CrawlSubTable

  stream:
    handler: bin/stream
    package:
      include:
        - bin/stream
    events:
      - stream:
          type: dynamodb
          arn: !GetAtt CrawlTable.StreamArn
          batchSize: 1 # TODO: Increase.
    environment:
      CRAWL_SUB_TABLE_NAME: CrawlSubTable
      CRAWL_SUB_GSI_NAME: CrawlIDIndex

  schedule:
    handler: bin/schedule
    package:
      include:
        - bin/schedule
    events:
      - http:
          path: crawl
          method: post
    cors: true
    environment:
      CRAWL_QUEUE_URL: !Ref CrawlQueue


resources:
  Resources:
    CrawlQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: "CrawlRequestQ"
        RedrivePolicy:
          deadLetterTargetArn: !GetAtt CrawlDeadLetterQueue.Arn
          maxReceiveCount: 1

    CrawlDeadLetterQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: "CrawlRequestDLQ"
        MessageRetentionPeriod: 7200 # 2 hours

    CrawlTable:
      Type: AWS::DynamoDB::Table
      Properties: 
        AttributeDefinitions:
          - AttributeName: "ParentURL"
            AttributeType: "S"
          - AttributeName: "CrawlID"
            AttributeType: "S"
        KeySchema: 
          - AttributeName: "CrawlID"
            KeyType: "HASH"
          - AttributeName: "ParentURL"
            KeyType: "RANGE"
        BillingMode: PAY_PER_REQUEST
        TableName: "CrawlTable"
        TimeToLiveSpecification:
          AttributeName: ExpirationDate
          Enabled: true
        StreamSpecification:
          StreamViewType: NEW_IMAGE

    CrawlSubscriptionTable:
      Type: AWS::DynamoDB::Table
      Properties: 
        AttributeDefinitions:
          - AttributeName: "ConnectionID"
            AttributeType: "S"
          - AttributeName: "CrawlID"
            AttributeType: "S"
        KeySchema: 
          - AttributeName: "ConnectionID"
            KeyType: "HASH"
          - AttributeName: "CrawlID"
            KeyType: "RANGE"
        BillingMode: PAY_PER_REQUEST
        TableName: "CrawlSubTable"
        GlobalSecondaryIndexes:
          - IndexName: CrawlIDIndex
            Projection:
              ProjectionType: ALL
            KeySchema: 
            - AttributeName: "CrawlID"
              KeyType: "HASH"

plugins:
  - serverless-sam
